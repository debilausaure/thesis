\chapter{Service de transfert de flot d'exécution avec preuve d'isolation}

Ce chapitre décrit la première contribution de cette thèse : un service de transfert de flôt d'exécution pour Pip. Ce chapitre commencera par exposer les motivations qui ont conduit à ce service de transfert de flôt d'exécution.

La seconde section décrira le service tel qu'il a été conçu : en premier lieu, nous exposerons le principe général derrière le service, en explicitant notamment les structures de données et le prototype du service. Cette exposition du service sera suivie d'une illustration de l'utilisation du service sur les trois différents transferts de flot d'exécution au sein d'un système : les appels systèmes entre différents espaces d'adressages, ainsi que les transferts de flot d'exécution suite à une faute ou une interruption. Cette section s'achèvera sur une vue interne du service, décrivant les différents blocs unifiant ces trois différents transferts.

La troisième section expliquera le processus de preuve du service, en commençant par la définition des types nécessaire à l'écriture du service et plus généralement de la conception des ajouts à l'interface avec la monade. Cette section détaillera ensuite les différentes propriétés d'isolation, puis identifiera les points délicats de l'établissement de la preuve en s'appuyant sur les différents blocs détaillés dans la section précédente.

La dernière section de ce chapitre reviendra sur la conception de ce service d'un point de vue pragmatique, en s'intéressant à quelques métriques et en revenant sur la pertinence de la preuve.

% Réecrire le modele de writeContext qui devrait écrire dans le modèle si la page donnée est une page noyau

	\section{Motivations}

		Point de vue pragmatique :
			- anciennement deux appels systèmes \texttt{dispatch} et \texttt{resume} disponibles, écrit en C sans documentation, qui ne couvraient pas la totalité des cas d'usage.
			  donc nécessité de (re-)conception d'un mécanisme de transfert de flot d'exécution car le changement d'espace d'adressage est une opération privilégiée.

		Point de vue académique :
			- compléter la preuve des appels système de Pip pour surenchérir sur la validité de la méthologie de Pip
			- valeur intrinsèque de l'unification des diférents transferts de flot de controle
		\subsection{Failles de sécurité}
		\subsection{Changement d'espace d'adressage opération privilégiée}
		\subsection{Arguments de co-design (minimaliste, générique)}
			

	\section{Description du service}

	Avant toute chose, il faut définir ce qu'on attend du service, sa spécification. En particulier, il s'agit de spécifier les transferts de flôt de contrôle valides au sein du système, que ce soit pour les transferts explicites tels que les appels systèmes ou les transferts implicites comme les fautes ou les interruptions. Cette spécification doit pouvoir accomoder tous les cas d'usage de transfert de flôt d'exécution au sein d'un noyau tel que Pip, conçu comme une tour de virtualisation.
	
	Pour rappel, Pip définit des partitions de mémoire qui sont responsables de la mémoire qui leur est attribuée. Chaque partition de mémoire a son propre espace d'adressage. Ces partitions peuvent engendrer des sous-partitions, en partageant en partie de leur propre mémoire. Les sous-partitions engendrées de cette manière sont appelées les partitions enfants. La partition ayant partagé sa mémoire avec son enfant est appelée la partition parent. Au démarrage du système, une seule partition est créée par Pip. Cette partition a accès à l'intégralité de la mémoire : c'est la partition racine.

	\subsubsection{Flôts d'exécution valides au sein d'une tour de virtualisation}

	Les transferts de flot de contrôle valides entre les différentes partitions reprennent les trois modalités présentées dans le chapitre précédent en section \ref{control_flow_transfer} en les voyant à travers le prisme d'une tour de virtualisation.

	La tour de virtualisation crée un système de délégation des fonctionnalités. L'intégralité des fonctions du système est initialement endossé par la partition racine, qui peut décider de déléguer certaines fonctionnalités à ses enfants. Les partitions enfants peuvent à leur tour déléguer ces fonctionnalités à leurs propres enfants ; la partition racine n'en a cependant pas forcément connaissance. C'est pourquoi les transferts de flôt d'exécution explicites ne sont nécessaires qu'entre parent et enfants ; chaque partition connait les fonctionnalités qui lui incombent, et peut donc diriger le flot d'exécution vers une autre partition si nécessaire. \textbf{Ainsi, chaque partition offre un certain nombre de services qui définissent son interface.}

	Lorsqu'une faute survient, une partition manque à ses responsabilités. La faute remonte la chaîne de responsabilité vers son parent qui peut alors gérer l'incident.

	Les interruptions matérielles signalent un évènement extérieur dont la responsabilité peut incomber à n'importe quelle partition, et seule la partition racine connait l'ensemble des chaînes de responsabilité. Ainsi, lorsqu'une interruption matérielle survient, la partition racine récupère le flôt d'exécution et peut -- si nécessaire -- diriger l'interruption vers la partition qui en a la responsabilité. Ceci est semblable à un superviseur muni d'une fonction de multiplexage.

Ainsi, même s'il existe un grand nombre de modalités de transfert de flôt d'exécution en pratique, l'architecture du proto-noyau Pip promeut un modèle unifié qui adapte à une tour de virtualisation les trois situations génériques. En réduisant à trois cas distincts l'ensemble des modalités de transferts de flot d'exécution, le travail de preuve de programme nécessaire pour établir une garantie de sécurité est simplifié. Cependant, la sous-section suivante s'attache à démontrer qu'il est possible de résumer ces trois cas distincts en un seul service dont la preuve de bon fonctionnement apporte les garanties de sécurité à l'ensemble des situations de transfert de flot d'exécution possibles au sein de l'architecture x86.

	\subsection{Principe de fonctionnement du service} 
	\label{service_idea}

	\subsubsection{Structures de données du service}

	\paragraph{VIDT} Les services exposés par les différentes partitions sont définis dans une structure appelée \emph{Virtual Interrupt Descriptor Table} ou \emph{VIDT}. Cette structure reprend les concepts de l'\emph{IDT} classique (voir \ref{IDT}), appliqués à chaque partition. Elle doit être placée - par convention - au début de la dernière page virtuelle de chaque partition. Cependant, contrairement à l'\emph{IDT} qui contient des \emph{gates} composées de pointeurs de fonctions et de contrôles de droits, la \emph{VIDT} de chaque partition contient des pointeurs vers des \emph{contextes} d'exécution, comme illustré sur la figure \ref{fig:vidt}.

\begin{figure}[!ht]
	\centering
	\input{figures/VIDT.tex}
	\caption{La structure d'une VIDT}
	\label{fig:vidt}
\end{figure}

	\paragraph{Contexte d'exécution} Ces \emph{contextes} sont des instantanés de l'état du processeur au moment du transfert du flot d'exécution. Pour l'architecture Intel x86, ils sont partiellement générés par les mécanismes du matériel tels que détaillé dans le chapitre précédent dans la section \ref{context}, puis complétés par du logiciel. Ces contextes d'exécution peuvent aussi être créés ex-nihilo par les partitions afin de définir de nouveaux services.


Plus simplement, les services de chaque partition sont définis dans leur \emph{VIDT} au moyen de pointeurs vers des contextes d'exécution. Il est important de noter que ces contextes d'exécution sont situés dans l'espace d'adressage de chaque partition, et sont donc \textbf{accessibles et modifiables} par le code non privilégié.

	\subsubsection{Principe d'utilisation et prototype du service pour un transfert de flôt d'exécution}
	\label{sec:service_usage}
	\begin{listing}[!ht]
		\ccode{code/entrypoint_prototype.c}
		\caption{Prototype du point d'entrée du service tel qu'appelée par les partitions}
		\label{code:c_proto}
	\end{listing}

	Lors d'un appel explicite au service de transfert de flôt d'exécution dont le protoype est donné par le listing \ref{code:c_proto}, la partition appelante doit désigner une autre partition ainsi que le numéro de service désiré. La partition est désignée l'adresse virtuelle de son descripteur correspondant au paramètre \texttt{calleePartDescVAddr}. Dans le cas d'un appel vers la partition parent, l'adresse par défaut est utilisée. Le numéro de service n'est autre que la position du pointeur vers le contexte d'exécution à restaurer dans la VIDT de la partition ciblée, correspondant au paramètre \texttt{userTargetInterrupt}. Ces deux paramètres permettent de déterminer où transférer le flôt d'exécution.

	De plus, Pip permet à la partition appelante de sauvegarder son contexte d'exécution actuel afin qu'il puisse être restauré et que l'exécution puisse reprendre ultérieurement. La partition appelante doit avoir réservé préalablement de la mémoire pour que Pip puisse y placer un contexte, et renseigné un pointeur vers cet espace dans sa propre VIDT. Pour que Pip préserve le contexte d'exécution, la partition doit fournir l'entier \texttt{userContextSaveIndex} qui indique la position du pointeur dans sa VIDT pointant vers l'espace réservé. Si un pointeur nul se trouve à la position indiquée, le contexte n'est pas sauvegardé.

	Les deux derniers paramètres, \texttt{flagsOnYield} et \texttt{flagsOnWake} permettent à la partition de restreindre l'utilisation de certains de ses services. Ce sont en réalité des drapeaux vérifiés par le service de transfert de flôt d'exécution de Pip indiquant que certains services de la partition sont temporairement indisponibles, bien qu'ils soient correctement configurés. \texttt{flagsOnYield} sont les drapeaux qui seront appliqués immédiatement par Pip à la partition appelante au moment du transfert de flôt d'exécution. \texttt{flagsOnWake} sont les drapeaux qui seront appliqués au moment de la restauration du contexte d'exécution actuel de la partition.

	Enfin, un dernier paramètre contenant un pointeur vers le contexte d'exécution est généré par le code trampoline permettant d'exécuter le code du service écrit en Gallina. Ceci permet au service de sauvegarder le contexte d'exécution comme énoncé précédemment. La figure \ref{code:gallina_proto} montre le prototype attendu par le code prouvé.

		\begin{listing}[!ht]
			\coqcode{code/prototype.v}
			\caption{Prototype du point d'entrée du service en Gallina}
			\label{code:gallina_proto}
		\end{listing}
		
		\subsection{Illustration de mise en place du service sur l'architecture Intel x86 au travers d'un appel explicite}

			\subsubsection{Point d'entrée par \texttt{callgate}}

		Dans l'implémentation de Pip sur l'architecture Intel x86, les services de Pip sont appelables au travers de \emph{callgates} (voir \ref{sec:x86_syscall}). Ces callgates permettent au code non privilégié des partitions d'appeler les services privilégiés de Pip.
		Pour ce faire, la partition doit pousser les arguments décrits en section \ref{sec:service_usage} sur sa pile, puis utiliser un \emph{farcall}. Cet appel est implémenté au sein de la LibPip, la librairie utilisateur facilitant l'utilisation du noyau. Son code est disponible en annexe (voir listing \ref{code:libpip_yield}).

		Lorsque le processeur exécute l'instruction \texttt{lcall} de la partition, le flôt d'exécution est transféré vers Pip et le processeur passe en mode privilégié, copie les paramètres et pousse partiellement l'état précédent sur la pile (voir \ref{sec:intel_callgate}). Dans le cas de l'appel au service de transfert de flôt d'exécution, le processeur commence par exécuter une routine qui va sauver sur la pile noyau le contexte d'exécution de la partition encore partiellement présent dans les registres. Accessoirement, cette routine réordonne les éléments de la pile afin de regrouper les différentes parties du contexte et de pouvoir utiliser une structure \texttt{gate\_ctx\_t} pour le représenter. Cette routine étant un peu longue, elle est placée en annexe (voir Fragment de code \ref{code:cg_yieldGlue}).
		La figure \ref{fig:cg_stack} montre l'état de la pile après l'exécution de la routine.

		\begin{figure}[!ht]
			\input{figures/cg_stack.tex}
			\caption{État de la pile du noyau après la routine assembleur exécutée après l'appel du service au travers d'une callgate}
			\label{fig:cg_stack}
		\end{figure}
	
		\paragraph{Harmonisation du contexte d'exécution et appel du service prouvé} \label{sec:context_harmonisation} Avant d'appeler le code prouvé, une dernière transformation est opérée sur le contexte d'exécution. Il est copié en haut de la pile, puis transformé en contexte générique de type \texttt{user\_ctx\_t} afin d'harmoniser les différentes représentations de contexte entre les différents points d'entrées du service. Le code est disponible en annexe (voir Fragment de code \ref{code:yieldGlue}).

			\subsubsection{Introduction générale des étapes du service}

			Le service écrit en Gallina permettant de transférer le flôt d'exécution procède en trois étapes.

			\paragraph{Étape préliminaire de validation et de récupération des données} Avant toute chose, la première étape du service vérifie la validité des arguments et des structures modifiables en espace utilisateur. En particulier, elle vérifie que l'adresse virtuelle fournie comme la cible du transfert correspond bien à une partition enfant ou parent, et récupère l'adresse réelle à laquelle débute son descripteur. Elle vérifie aussi que les \emph{VIDT} des partitions appelantes et appelées sont accessibles en espace utilisateur, et que les espaces de mémoires ciblés par l'appel sont eux aussi accessibles. Ceci permet par exemple de récupérer le contexte d'exécution de la partition ciblée par l'appel. Cette étape préliminaire permet de s'assurer que le service ne pourra pas rencontrer d'erreur dans les prochaines étapes.

			\paragraph{Étape de modification de l'état} La seconde partie du service est une étape procédant à la modification de l'état du système. Cette étape regroupe toutes les écritures en mémoire requises par le service. Tout d'abord, le service va procéder à l'écriture du contexte de la partition appelante dans son espace d'adressage (si demandé lors de l'appel). Le contexte est recopié depuis la pile noyau jusqu'à la zone de mémoire pointée par le pointeur dans la \emph{VIDT} de la partition appelante.
			Ensuite, le service met à jour l'espace d'adressage pour refléter l'espace d'adressage de la partition cible.
			Enfin, le service procède à la mise à jour des structures de données du noyau afin de préserver les propriétés de cohérence internes de Pip. Cette étape ne nécessite en fait qu'une unique écriture dans une variable globale, indiquant quelle partition s'exécutera lorsque le flôt d'exécution repassera en espace utilisateur.

			\paragraph{Étape de transfert de flôt d'exécution} La troisième et dernière étape du service transfère le flôt d'exécution vers la partition appelée au travers du contexte d'exécution récupéré lors de l'étape préliminaire. Cette étape est représentée dans le code prouvé par un appel à une fonction de l'interface.

		\subsection{Décomposition des opérations}

		Afin de pouvoir accomoder les différents modes de transfert de flôt d'exécution au sein d'un même service, le service est composé de plusieurs blocs de code remplissant leur propre fonction et appelant le bloc de code suivant. Ces blocs pourraient être assimilés à des \emph{continuations}. Les blocs composants le service sont illustrés à la fin de la sous-section sur la figure \ref{fig:callgraph}.

		\paragraph{Vérification du numéro de contexte ciblé}

		Le bloc de code \texttt{checkIntLevelCont} est le point d'entrée dans le service lors d'un appel explicite, tel que présenté dans le code \ref{code:gallina_proto}.
		Il se contente de vérifier que le numéro de contexte ciblé soit bien valide et transforme son type pour qu'il soit utilisable par le noyau. Il appelle ensuite le bloc de code \texttt{checkCtxSaveIdxCont}.

		%\begin{listing}[!ht]
		%	\coqcode{code/checkIntLevelCont.v}
		%	\caption{Prototype du point d'entrée du service en Gallina}
		%	\label{code:checkIntLevelCont}
		%\end{listing}

		\paragraph{Vérification du numéro de sauvegarde du contexte de la partition appelante}
		
		Le bloc de code \texttt{checkCtxSaveIdxCont} vérifie que le numéro de sauvegarde du contexte de la partition appelante est valide, et transforme son type pour qu'il soit utilisable par le noyau. Il récupère ensuite le descripteur de partition de la partition appelante, ainsi que son Page Directory (la page mémoire racine de la configuration de son espace d'adressage -- voir \ref{sec:intel_mmu}).

		Enfin, ce bloc de code va vérifier si la valeur de l'adresse virtuelle désignant la partition appelée est celle par défaut. Si c'est le cas, le prochain bloc de code sera \texttt{getParentPartDescCont}. Si ce n'est pas la valeur par défaut, il va appeler le bloc de code \texttt{getChildPartDescCont}.

		%\begin{listing}[!ht]
		%	\coqcode{code/checkCtxSaveIdxCont.v}
		%	\caption{Prototype du point d'entrée du service en Gallina}
		%	\label{code:checkCtxSaveIdxCont}
		%\end{listing}

		\paragraph{Récupération de la partition parent}

		Le bloc de code \texttt{getParentPartDescCont} est un des deux blocs d'exécution possibles concernant la cible du transfert de flôt d'exécution qui traite de l'appel vers le parent. Il vérifie uniquement que la partition appelante n'est pas la partition racine, puisque la partition racine n'a pas de parent. Il récupère ensuite le descripteur de partition de son parent, qu'il passera en tant que descripteur de partition appelée au prochain bloc de code \texttt{getSourceVidtCont}. Ce prochain bloc de code est commun aux deux fils d'exécutions alternatifs.

		%\begin{listing}[!ht]
		%	\coqcode{code/getParentPartDescCont.v}
		%	\caption{Prototype du point d'entrée du service en Gallina}
		%	\label{code:getParentPartDescCont}
		%\end{listing}

		\paragraph{Récupération de la partition enfant}

		Le bloc de code \texttt{getChildPartDescCont} est l'autre alternative d'exécution du transfert de flôt d'exécution qui traite de l'appel vers un enfant. Il vérifie que l'adresse virtuelle passée comme paramètre correspond au descripteur de partition d'une partition enfant. Pour le vérifier, Pip vérifie tout d'abord que l'adresse virtuelle réside bien dans l'espace d'adressage de la partition appelante, et vérifie ensuite dans ses structures de données internes que l'adresse correspond à un descripteur de partition.

		Ensuite, de la même manière que le bloc de code concernant l'appel à un parent, ce bloc de code passe le descripteur de partition au prochain bloc de code \texttt{getSourceVidtCont}.

		%\begin{listing}[!ht]
		%	\coqcode{code/getChildPartDescCont.v}
		%	\caption{Prototype du point d'entrée du service en Gallina}
		%	\label{code:getChildPartDescCont}
		%\end{listing}

		\paragraph{Récupération de la VIDT de la partition appelante}
		\label{sec:getSourceVidtCont}

		Le bloc de code \texttt{getSourceVidtCont} vérifie d'abord que la VIDT de la partition appelante est mappée et accessible dans son espace d'adressage. Une fois qu'il a déterminé qu'il était possible de lire dans la VIDT, il va récupérer l'adresse virtuelle de l'espace mémoire où sauvegarder le contexte de la partition appelante. Enfin, la page de mémoire contenant la VIDT, et l'adresse virtuelle de l'espace mémoire de sauvegarde sont passés au bloc de code suivant \texttt{getTargetVidtCont}.

		%\begin{listing}[!ht]
		%	\coqcode{code/getSourceVidtCont.v}
		%	\caption{Prototype du point d'entrée du service en Gallina}
		%	\label{code:getSourceVidtCont}
		%\end{listing}

		\paragraph{Récupération de la VIDT de la partition ciblée}
		\label{sec:getTargetVidtCont}
		Tout d'abord, le bloc de code \texttt{getTargetVidtCont} va récupérer le \emph{Page Directory} de la partition appelée à partir de son descripteur de partition. Ensuite, similairement au bloc de code précédent, il vérifie que la VIDT de la partition appelée est mappée et accessible dans son espace d'adressage. Ensuite, la page de mémoire contenant la VIDT ainsi que son \emph{Page Directory} sont passés au bloc de code suivant \texttt{getTargetContextCont}.
		%\begin{listing}[!ht]
		%	\coqcode{code/getTargetVidtCont.v}
		%	\caption{Prototype du point d'entrée du service en Gallina}
		%	\label{code:getTargetVidtCont}
		%\end{listing}

		\paragraph{Récupération du contexte de la partition ciblée}

		Le bloc de code \texttt{getTargetContextCont} vérifie que le pointeur vers le contexte d'exécution ciblé pointe bien dans l'espace d'adressage de la partition ciblée, et que cet espace est bien accessible à la partition. En supplément, le bloc va vérifier que l'adresse de fin du contexte ne va pas overflow, et que la dernière adresse du contexte est aussi accessible à la partition afin de s'assurer que l'entièreté du contexte pourra être lu sans déclencher de faute. Ceci introduit l'hypothèse qu'un contexte d'exécution a une taille inférieure à une page mémoire.
		Enfin, le bloc de code va comparer l'adresse virtuelle de sauvegarde de contexte récupérée au bloc \texttt{getSourceVidtCont} (voir \ref{sec:getSourceVidtCont}) avec l'adresse virtuelle par défaut. Si cette adresse n'est pas celle par défaut, c'est que la partition veut sauvegarder son contexte , et le bloc de code \texttt{saveSourceContextCont} est appelé. Sinon, le bloc de code \texttt{switchContextCont} est appelé.
		%\begin{listing}[!ht]
		%	\coqcode{code/getTargetContextCont.v}
		%	\caption{Prototype du point d'entrée du service en Gallina}
		%	\label{code:getTargetContextCont}
		%\end{listing}

		\paragraph{Sauvegarde du contexte de la partition appelante}

		Le bloc de code \texttt{saveSourceContextCont} va se contenter de vérifier que l'espace mémoire pointé par l'adresse récupérée dans la VIDT de la partition appelante est bien dans l'espace d'adressage de la partition appelante et qu'il est accessible en espace utilisateur. Comme dans le bloc précédent, il va vérifier en supplément que l'adresse de fin de cet espace mémoire n'overflow pas, et qu'il reste accessible dans son espace d'adressage, afin de s'assurer qu'une écriture dans cette zone mémoire ne déclenchera pas de faute. Une fois les vérifications faites, il écrit le contexte de la partition appelante dans la zone mémoire, puis appelle le dernier bloc de code \texttt{switchContextCont}.
		%\begin{listing}[!ht]
		%	\coqcode{code/saveSourceContextCont.v}
		%	\caption{Prototype du point d'entrée du service en Gallina}
		%	\label{code:saveSourceContextCont}
		%\end{listing}

		\paragraph{Changement d'espace d'adressage et chargement du contexte d'exécution}

		Le bloc de code \texttt{switchContextCont} ne procède plus à aucune vérification. Il commence par écrire les drapeaux \texttt{flagsOnYield} dans la partition appelante. Ensuite, l'espace d'adressage est changé par celui de la partition appelée -- son \emph{Page Directory} ayant été récupéré dans le bloc de code \texttt{getTargetVidtCont} (voir \ref{sec:getTargetVidtCont}). Ensuite, il met à jour la variable de Pip indiquant la partition qui s'exécutera lorsque le processeur repassera en mode utilisateur. Le bloc de code récupère ensuite les drapeaux \texttt{flagsOnWake} du contexte d'exécution de la partition à réveiller et les applique à la partition courante, sur le nouvel espace d'adressage. À partir de ces drapeaux, il détermine si la partition appelée peut demander à ne pas être interrompue, puis procède enfin au chargement de son contexte.
		%\begin{listing}[!ht]
		%	\coqcode{code/switchContextCont.v}
		%	\caption{Prototype du point d'entrée du service en Gallina}
		%	\label{code:switchContextCont}
		%\end{listing}

		\newpage

		\begin{figure}[!ht]
			\input{figures/callgraph.tex}
			\caption{Vue éclatée des blocs constituant le service}
			\label{fig:callgraph}
		\end{figure}
		\newpage

		\subsection{Généralisation du service aux fautes et aux interruptions}

		L'idée principale derrière cette unification est qu'il est possible pour Pip de fixer certains paramètres et de commencer l'exécution à un endroit arbitraire du service. Il est notamment possible de commencer l'exécution directement après la validation des paramètres et de la traduction de l'adresse virtuelle de la partition cible. Ceci permet au système de passer en arguments les adresses réelles de partitions qui seront la cible des différents événements. La figure \ref{fig:callgraph} montre comment sont placés les différents points d'entrée du système dans le service.

		Cependant, les différents mécanismes décrits dans la section \ref{sec:x86_syscall} ne présentent pas d'interface commune ; c'est pourquoi, de petites portions de code C et assembleur sont placées juste avant les différents points d'entrée afin d'harmoniser les différents formats de contexte. Ces morceaux d'assembleur sont placés dans l'\emph{IDT} du système, afin qu'ils soient appelés lors d'une faute ou d'une interruption. Deux niveaux d'interruption sont réservés à la sauvegarde des contextes par Pip lorsqu'une ifaute ou une interruption survient. Sur l'architecture \texttt{x86} ce sont les niveaux 48 et 49 qui sont utilisés (les niveaux d'interruptions 0 à 31 sont réservés par Intel pour les fautes, les niveaux d'interrruptions de 32 à 47 ont été configurés pour correspondre aux interruptions matérielles).

		\subsubsection{Implémentation des fautes utilisant le service sur l'architecture x86}

		Pour rappel, les fautes doivent être transmises au parent de la partition fautive. Il est donc naturel de commencer l'exécution du service au bloc de code \texttt{getParentPartDescCont}. Le prototype de ce bloc de code est présenté en listing \ref{code:getParentPartDescCont}.

		\begin{listing}[!ht]
			\coqcode{code/getParentPartDescCont.v}
			\caption{Prototype du point d'entrée du service en Gallina}
			\label{code:getParentPartDescCont}
		\end{listing}

		Cependant, l'appel de ce bloc de code n'est pas trivial après une faute. Il s'agit d'abord de récupérer le contexte d'exécution de la partition fautive. Comme expliqué précédemment dans la section \ref{sec:faults}, lorsqu'une faute survient, le processeur va chercher la \emph{gate} installée dans l'\emph{IDT} dont le numéro correspond au niveau de la faute. Dans Pip, ces \emph{gates} sont toutes des \emph{interrupt gates}, qui s'exécutent en mode privilégié.

		De ce fait, le processeur change de pile. Il pousse le segment de pile \texttt{SS} ainsi que l'ancien pointeur vers le sommet de la pile \texttt{ESP}. Il pousse ensuite l'état des drapeaux du processeur \texttt{EFLAGS}, puis pousse le segment de code \texttt{CS} et le pointeur d'instruction \texttt{EIP} au moment de la faute. Enfin, en fonction de la faute qui a été déclenchée, le processeur peut éventuellement pousser un entier précisant la cause de la faute. La figure \ref{fig:proc_interrupt_stack} illustre l'état de la pile noyau après qu'une faute soit survenue.

		\begin{figure}[!ht]
			\input{figures/proc_interrupt_stack.tex}
			\caption{État de la pile noyau après qu'une faute soit survenue en espace utilisateur\\Reproduction partielle du manuel Intel \cite{intel_interrupt_stack}}
			\label{fig:proc_interrupt_stack}
		\end{figure}

		Pour unifier la structure des données sur la pile du noyau entre toutes les fautes et interruptions, le bout d'assembleur s'exécutant en sortie de faute va pousser une valeur de bourrage sur la pile si le processeur n'a pas poussé de code d'erreur. Il va pousser le niveau d'interruption de la faute sur la pile à des fins informatives. La routine assembleur va poursuivre en complétant le contexte d'exécution qui avait été partiellement sauvé par le processeur en poussant les registres généraux sur la pile. Ceci complète la structure \texttt{int\_ctx\_t} représentant le contexte d'exécution de la partition fautive. Enfin, le bout d'assembleur va pousser un pointeur vers cette structure. Cette routine assembleur s'achève en appelant le code C qui sera chargé de récupérer les arguments pour appeler le bloc de code \texttt{getParentPartDescCont}.

		\begin{figure}[!ht]
			\input{figures/interrupt_stack.tex}
			\caption{État de la pile noyau après qu'une faute soit survenue en espace utilisateur\\Reproduction partielle du manuel Intel \cite{intel_interrupt_stack}}
			\label{fig:interrupt_stack}
		\end{figure}

		Cette fonction s'appelle \texttt{faultInterruptHandler} ; son prototype est donné dans le listing \ref{code:faultInterruptHandler_proto}. Son code est trop long pour être inclus dans ce chapitre mais vous pouvez le retrouver en annexe \ref{code:faultInterruptHandler}.

		\begin{listing}[!ht]
			\ccode{code/faultInterruptHandler_proto.c}
			\caption{Prototype de la fonction calculant les arguments du service lors d'une faute}
			\label{code:faultInterruptHandler_proto}
		\end{listing}

		Tout comme l'appel du service par la \emph{callgate}, la fonction \texttt{faultInterruptHandler} commence par créer un nouveau contexte générique de type \texttt{user\_ctx\_t} à partir du contexte fautif précédemment créé. Ceci permet d'avoir la même représentation du contexte entre les différents points d'entrée du service, comme évoqué dans la section \ref{sec:context_harmonisation}. Cependant, le niveau de faute sera utilisé par la fonction comme argument \texttt{targetInterrupt}, pour que le parent soit reveillé avec le contexte lié à la faute.
		La fonction récupère ensuite le descripteur de partition fautive grâce à la variable globale de Pip indiquant la partition s'exécutant en espace utilisateur, qui deviendra l'argument \texttt{sourcePartDesc}. Elle récupère aussi l'état de ses drapeaux, liés aux arguments \texttt{flagsOnYield} et \texttt{flagsOnWake}. La fonction va décider où sauvegarder le contexte de la partition fautive en fonction de ces drapeaux. L'implémentation actuelle considère que si le mot mémoire représentant les drapeaux est égal à zéro, la partition ne souhaitait pas être interrompue (on parle de \emph{Virtual CLI}, en référence à l'instruction assembleur désactivant les interruptions). L'état sera alors sauvegardé dans l'espace mémoire pointé à l'index \texttt{CLI\_SAVE\_INDEX}. Si les drapeaux sont différents de zéro, alors l'autre indice réservé, \texttt{STI\_SAVE\_INDEX} sera utilisé pour l'argument \texttt{sourceContextSaveIndex}.
		Enfin, la fonction récupère le \emph{Page Directory} de la partition à partir de son descripteur qui servira d'argument \texttt{sourcePageDir}.

		Lorsque tous les arguments ont été récupérés\footnote{l'argument \texttt{nbL} de l'appel a été omis du texte principal car il est peu intéressant. Cet argument est en fait un paramètre de l'implémentation précisant le nombre d'indirections dans les structures de configurations de la MMU, soit 2 dans l'implémentation Intel x86}, la fonction est prête à appeler le bloc de code \texttt{getParentPartDescCont}.

		\paragraph{Cas des doubles fautes} Il est possible que l'appel au service échoue lors des vérifications pour le transfert de flôt d'exécution. Lorsque ce transfert est dû à une faute, la partition l'ayant déclenchée n'est plus en mesure de recevoir le flôt d'exécution : elle déclencherait à nouveau la faute. Ainsi, il faut que le service propose une voie de transfert de flôt d'exécution dont le succès ne dépend que de la bonne configuration de la partition ayant la responsabilité de la partition fautive -- c'est à dire son parent, la partition recevant le flot d'exécution.
		Dans le cas où le parent ne serait pas non plus en mesure de recevoir le flôt d'exécution, le flôt d'exécution serait redirigé vers le parent de la partition parent et remonterai la chaîne de responsabilité, jusqu'à ce qu'une partition reçoive le flôt d'exécution sans erreur, ou jusqu'à ce que la partition racine elle même échoue à le recevoir.

		C'est pourquoi la fonction récupérant les arguments appelle à son tour une autre fonction appelée \texttt{propagateFault} (cette fonction est disponible en annexe, voir listing \ref{code:faultInterruptHandler}). Cette fonction est chargée de faire l'appel au bloc de code \texttt{getParentPartDescCont} et de gérer les éventuelles erreurs pouvant survenir après un appel à ce bloc. Il y a trois cas d'erreurs distincs gérés par la fonction :
		\begin{itemize}
			\item le cas où la fonction ayant réalisé la faute est la partition racine : dans ce cas là, il n'y a plus rien à rattraper, la partition racine étant la base de confiance absolue du système -- si elle échoue le système s'écroule. Le service s'arrête alors sur une boucle infinie.
			\item le cas où le service n'a pas réussi à récupérer la VIDT de la partition fautive, ou l'espace mémoire permettant de sauvegarder le contexte : dans ce cas, la sauvegarde du contexte de la partition fautive est omise, et l'exécution reprend au bloc de code \texttt{getTargetVidtCont}.
			\item dans tous les autres cas d'erreur, la partition parent n'est pas correctement configurée et n'est pas en mesure de rattraper la faute. Dans ce cas, la fonction \texttt{propagateFault} fait un appel récursif. La faute est redirigée sur le parent de la cible actuelle, et le niveau d'interruption de la faute est changé au niveau correspondant à la double faute.
		\end{itemize}

		

		\subsubsection{Implémentation des interruptions utilisant le service sur l'architecture x86}

	Par exemple, lors d'une interruption matérielle survenant en espace utilisateur, le processeur va changer de niveau de privilèges et changer de pile, puis sauver quelques registres sur le sommet de la pile (\texttt{SS}, \texttt{ESP}, \texttt{EFLAGS}, \texttt{CS}, \texttt{EIP} - voir sous-section \ref{intel_hard_context}). Le code de gestion d'interruption pousse à son tour les registres généraux sur le sommet de pile, complétant la structure de \emph{contexte}. Ces quelques lignes d'assembleurs seront détaillées dans la sous-section suivante.
	%Les partitions peuvent \emph{déclarer} qu'elles ne souhaitent pas être interrompues, appelé \emph{état d'interruption} des partitions dans ce paragraphe. Ce mécanisme est analogue aux instructions \texttt{cli} et \texttt{sti} d'Intel ; cependant, les partitions indiquant qu'elles ne souhaitent pas être interrompues \textbf{le seront, peu importe leur état d'interruption}. L'état d'interruption sert uniquement à indiquer à la partition parente qu'il est nécessaire que le contexte courant de son enfant soit restauré en priorité. Il incombe donc au parent de s'assurer de restaurer ce contexte, plutôt qu'un autre. Pour cela, il existe un appel système permettant à un parent de récupérer l'état d'interruption de ses partitions enfants.


	\section{Preuve d'isolation}
		\subsection{Définition de l'interface/monade}
			% choix des types (générique en fonction des architectures - contextes)
			% limite de la preuve (écritures atomiques / conceptuelles)
		\subsection{Rappel? des propriétes d'isolation}
		
		\subsection{Déroulement de la preuve}
			\subsubsection{Validation des paramètres}
			\subsubsection{Modification de l'état}
			\subsubsection{Transfert de flot d'exécution}

	\section{Retour d'expérience}
	% Remarques pragmatiques sur cette contribution
		\subsection{Métriques}
		\subsection{Prise de recul sur la nature de la preuve}
		\subsection{Limites du service}
			- parler de la limite de la taille des flags comparé à nombre d'interruptions
